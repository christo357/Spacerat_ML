{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "We begin with the usual setup that we've seen in the previous pytorch code."
      ],
      "metadata": {
        "id": "xBWWX62yOEWY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79AR4JOpkNQf",
        "outputId": "0ec14d72-386c-4cea-ab9d-37d252527f61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 11.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 349kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 3.18MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 4.22MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "\n",
        "trainset = datasets.MNIST(root='./data', train=True, download=True, transform = transforms.ToTensor())\n",
        "testset = datasets.MNIST(root='./data', train=False, download=True, transform = transforms.ToTensor())\n",
        "\n",
        "x_train = (trainset.data / 256) - 0.5\n",
        "y_train = trainset.targets\n",
        "\n",
        "x_test = (testset.data / 256) - 0.5\n",
        "y_test = testset.targets\n",
        "\n",
        "def confusion_matrix(model, x, y):\n",
        "  model.eval()\n",
        "  identification_counts = np.zeros( shape = (10,10), dtype = np.int32 )\n",
        "\n",
        "  logits, probabilities = model( x ) # Note the assumption here that model will be returning logits and probabilities, so we need to build our models accordingly\n",
        "  predicted_classes = torch.argmax( probabilities, dim = 1 )\n",
        "\n",
        "  n = x.shape[0]\n",
        "\n",
        "  for i in range(n):\n",
        "    actual_class = y[i]\n",
        "    predicted_class = predicted_classes[i].item()\n",
        "\n",
        "    identification_counts[ actual_class, predicted_class ] += 1\n",
        "\n",
        "  total_correct = 0\n",
        "  for i in range(10):\n",
        "    total_correct += identification_counts[i, i] # This is new - note that the diagonal elements of the confusion matrix represent correct classifications, so that the sum is the total correct classified\n",
        "\n",
        "  accuracy = total_correct / np.sum( identification_counts ) # Accuracy is total correct / total tried\n",
        "  return identification_counts, accuracy\n",
        "\n",
        "def get_batch( x, y, batch_size ):\n",
        "  n = x.shape[0]\n",
        "  batch_indices = random.sample( [ i for i in range(n) ], k = batch_size )\n",
        "  x_batch = x[ batch_indices ]\n",
        "  y_batch = y[ batch_indices ]\n",
        "\n",
        "  return x_batch, y_batch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_function = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "66EELrZnqYft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MNISTNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MNISTNetwork, self).__init__()\n",
        "\n",
        "    self.layer_1 = torch.nn.Linear( in_features = 784, out_features = 170, bias = True ) # Choice of 170 here is arbitrary for the number of hidden nodes in that middle layer\n",
        "    self.layer_2 = torch.nn.Linear( in_features = 170, out_features = 10, bias = True)\n",
        "\n",
        "  def forward(self, input_tensor):\n",
        "    flattened = nn.Flatten()( input_tensor )\n",
        "\n",
        "    hidden_nodes = self.layer_1( flattened )\n",
        "    hidden_nodes = torch.nn.Sigmoid()( hidden_nodes )\n",
        "\n",
        "    logits = self.layer_2( hidden_nodes )\n",
        "    final_probabilities = nn.Softmax( dim = 1 )( logits )\n",
        "\n",
        "    return logits, final_probabilities\n",
        "\n",
        "class CNN_MNISTNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN_MNISTNetwork, self).__init__()\n",
        "\n",
        "    self.conv_1 = torch.nn.Conv2d( in_channels = 1, out_channels = 20, kernel_size = (3,3), stride = 1, bias = True )\n",
        "\n",
        "    # 28 x 28 pixel image\n",
        "    # how many places can I apply a 3x3 window at stride 1? 26\n",
        "    # We get 26 x 26 'local nodes'\n",
        "    # Layer output : 26 x 26 x 20\n",
        "\n",
        "    # Best way to think of this - at each of the 26x26 possible locations where a window can be applied, we are computing 20 features based on the pixels ther\n",
        "\n",
        "    self.linear = torch.nn.Linear( in_features = 26*26*20, out_features = 10, bias = True )\n",
        "    # We want to take the total computed features, flatten them, and treat them as input to a softmax layer.\n",
        "\n",
        "  def forward(self, input_tensor):\n",
        "    # Input tensor : shape [ N x 28 x 28 ]\n",
        "    # Reshape: [ N x 1 x 28 x 28 ]\n",
        "\n",
        "    reshaped = torch.reshape( input_tensor, (-1, 1, 28, 28) )\n",
        "\n",
        "    conv_results = self.conv_1( reshaped ) # output an [ N x 20 x 26 x 26 ] block\n",
        "\n",
        "    conv_results = torch.nn.Sigmoid()( conv_results ) # Apply sigmoid for nonlinear features\n",
        "\n",
        "    flattened = torch.nn.Flatten()( conv_results ) # [ N x (20*26*26) ]\n",
        "\n",
        "    logits = self.linear( flattened )\n",
        "\n",
        "    probabilities = nn.Softmax( dim = 1 )( logits )\n",
        "    return logits, probabilities"
      ],
      "metadata": {
        "id": "h2Kkdr4TlQ84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MNISTNetwork()\n",
        "cnn_model = CNN_MNISTNetwork()\n",
        "\n",
        "optimizer = optim.SGD( model.parameters(), lr = 0.01 )\n",
        "cnn_optimizer = optim.SGD( cnn_model.parameters(), lr = 0.01 )"
      ],
      "metadata": {
        "id": "OEvKC6WnprO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the training loop below, we perform 10 passes through the data, simultaneously updating the vanilla NN and the CNN. Losses are printed simultaneously for easy comparison."
      ],
      "metadata": {
        "id": "1SQ1ixFYPeuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1024\n",
        "model.train()\n",
        "cnn_model.train()\n",
        "\n",
        "for epochs in range(10):\n",
        "  total_loss = 0\n",
        "  cnn_total_loss = 0\n",
        "\n",
        "  for batch in range( 60000 // batch_size ):\n",
        "    x_batch, y_batch = get_batch( x_train, y_train, batch_size )\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    cnn_optimizer.zero_grad()\n",
        "\n",
        "    logits, probabilities = model( x_batch )\n",
        "    cnn_logits, cnn_probabilities = cnn_model( x_batch )\n",
        "\n",
        "    loss = loss_function( logits, y_batch )\n",
        "    cnn_loss = loss_function( cnn_logits, y_batch )\n",
        "\n",
        "    loss.backward()\n",
        "    cnn_loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "    cnn_optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "    cnn_total_loss += cnn_loss.item()\n",
        "\n",
        "  print(\"Average Loss per Data Point (Reg/CNN):\", total_loss / ( 60000 // batch_size ), cnn_total_loss / ( 60000 // batch_size ) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlArzVBqp8oh",
        "outputId": "2ea6ddc1-f4f2-4a02-e594-ebcb888c5e83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Loss per Data Point (Reg/CNN): 2.3038760382553627 4.558437980454544\n",
            "Average Loss per Data Point (Reg/CNN): 2.2778756043006636 3.812012425784407\n",
            "Average Loss per Data Point (Reg/CNN): 2.2634251035492996 2.813050613321107\n",
            "Average Loss per Data Point (Reg/CNN): 2.2477537311356643 2.026924421047342\n",
            "Average Loss per Data Point (Reg/CNN): 2.2319094559241988 1.5435694969933609\n",
            "Average Loss per Data Point (Reg/CNN): 2.215120928040866 1.2005054467710956\n",
            "Average Loss per Data Point (Reg/CNN): 2.1974128238086044 1.009103757554087\n",
            "Average Loss per Data Point (Reg/CNN): 2.179364829227842 0.8453050379095406\n",
            "Average Loss per Data Point (Reg/CNN): 2.1574009862439385 0.7575626630207588\n",
            "Average Loss per Data Point (Reg/CNN): 2.136398747049529 0.7137375463699472\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see from the above that while both losses decrease steadily, the performance of the CNN is rapidly superior to the vanilla NN.\n",
        "\n",
        "The reason for this is that the features that are relevant in computer vision /are/ local and spatially invariant. The CNN does not have to waste time learning this, as we have hard-coded it to learn those kinds of features."
      ],
      "metadata": {
        "id": "mbpzArN2PqhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix(model, x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LT4ZPcHcq7Hj",
        "outputId": "9737cefd-fd97-4b16-c7e7-ac85340b0c86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[ 931,   22,    6,   10,    0,    0,    8,    2,    1,    0],\n",
              "        [   0, 1131,    1,    2,    0,    0,    1,    0,    0,    0],\n",
              "        [  50,  380,  558,   18,    3,    0,    5,   15,    3,    0],\n",
              "        [  27,  245,   23,  703,    0,    0,    0,   11,    1,    0],\n",
              "        [  25,  335,    6,   17,  279,    0,   32,  287,    1,    0],\n",
              "        [ 120,  324,    4,  352,    0,    3,   17,   66,    0,    6],\n",
              "        [  88,  207,    8,    7,    1,    0,  640,    7,    0,    0],\n",
              "        [   6,  232,    5,    2,    0,    0,    0,  782,    1,    0],\n",
              "        [  63,  550,   14,  143,    2,    0,    9,   31,  160,    2],\n",
              "        [  44,  287,    9,   33,   15,    0,    6,  498,    1,  116]],\n",
              "       dtype=int32),\n",
              " 0.5303)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix(cnn_model, x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIoKlF5lr6FB",
        "outputId": "a84b975f-56f7-48fa-f272-67ca6d6b9509"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[ 938,    0,    1,   11,    3,    0,   20,    1,    6,    0],\n",
              "        [   0, 1078,    3,   14,    3,    0,    5,    1,   31,    0],\n",
              "        [  20,   27,  762,   70,   33,    0,   34,   32,   54,    0],\n",
              "        [   4,    6,   12,  932,    2,    2,    5,   23,   24,    0],\n",
              "        [   1,   11,    2,    4,  914,    1,   31,    5,    9,    4],\n",
              "        [  23,   42,    1,  255,   61,  380,   36,   40,   54,    0],\n",
              "        [  19,    9,   10,    4,   20,    9,  882,    1,    4,    0],\n",
              "        [   3,   35,   18,    6,   18,    0,    1,  933,   13,    1],\n",
              "        [  13,   20,    4,   85,   16,    4,   21,   18,  792,    1],\n",
              "        [  20,   17,    6,   31,  469,    3,    8,  182,   49,  224]],\n",
              "       dtype=int32),\n",
              " 0.7835)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating on the test data, we see a significantly improved accuracy, even with the exact same number of passes through the data."
      ],
      "metadata": {
        "id": "meAi3uU6P_TO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for parameter in cnn_model.conv_1.named_parameters():\n",
        "  print( parameter[0] )\n",
        "  print( parameter[1].shape )"
      ],
      "metadata": {
        "id": "zwNuWMg5r9d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ef79c5c-f8f4-4f6b-abe4-d9be6d7c783e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weight\n",
            "torch.Size([20, 1, 3, 3])\n",
            "bias\n",
            "torch.Size([20])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of the above is simply to illustrate what the CNN is doing - it has weights that it is applying on a 3x3 window, and it has 20 such weights, giving a total 'weight block' of 20x3x3. This block of weights is applied at multiple locations, so that the same features are computed at different locations in the input.\n",
        "\n",
        "Note that the bias is a vector with 20 values - each value representing the bias constant for that specific feature."
      ],
      "metadata": {
        "id": "XmZYr3WaQGFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_MNISTNetwork_Big(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN_MNISTNetwork_Big, self).__init__()\n",
        "\n",
        "    self.conv_1 = torch.nn.Conv2d( in_channels = 1, out_channels = 20, kernel_size = (3,3), stride = 1, bias = True )\n",
        "    self.conv_2 = torch.nn.Conv2d( in_channels = 20, out_channels = 20, kernel_size = (2,2), stride = 2, bias = True ) # Adding a second convolutional layer\n",
        "\n",
        "    # Note the progression here\n",
        "    # Input will be [ N x 1 x 28 x 28 ]\n",
        "    # After the first convolutional layer, [ N x 20 x 26 x 26 ] since there are 26 places to put a 3x3 window at stride 1\n",
        "    # After the second convolutional layer, [ N x 20 x 13 x 13 ] since there are 13 places to put down a 2x2 window at stride 2\n",
        "\n",
        "    self.linear = torch.nn.Linear( in_features = 13*13*20, out_features = 10, bias = True ) # Dump all the features into a linear layer to get the logits\n",
        "\n",
        "  def forward(self, input_tensor):\n",
        "    reshaped = torch.reshape( input_tensor, (-1, 1, 28, 28) )\n",
        "\n",
        "    conv_results = self.conv_1( reshaped )\n",
        "    conv_results = torch.nn.ReLU()( conv_results ) # NOTE: Based on results in class, ReLU gave us a significant speed up and performance improvement\n",
        "\n",
        "    conv_results = self.conv_2( conv_results )\n",
        "    conv_results = torch.nn.ReLU()( conv_results )\n",
        "\n",
        "    flattened = torch.nn.Flatten()( conv_results )\n",
        "\n",
        "    logits = self.linear( flattened )\n",
        "\n",
        "    probabilities = nn.Softmax( dim = 1 )( logits )\n",
        "    return logits, probabilities"
      ],
      "metadata": {
        "id": "XYTHIUlQlmhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model = CNN_MNISTNetwork_Big()\n",
        "\n",
        "cnn_optimizer = optim.Adam( cnn_model.parameters(), lr = 0.01 )"
      ],
      "metadata": {
        "id": "wAngqWulnKgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1024\n",
        "cnn_model.train()\n",
        "\n",
        "for epochs in range(10):\n",
        "  cnn_total_loss = 0\n",
        "\n",
        "  for batch in range( 60000 // batch_size ):\n",
        "    x_batch, y_batch = get_batch( x_train, y_train, batch_size )\n",
        "\n",
        "    cnn_optimizer.zero_grad()\n",
        "\n",
        "    cnn_logits, cnn_probabilities = cnn_model( x_batch )\n",
        "\n",
        "    cnn_loss = loss_function( cnn_logits, y_batch )\n",
        "\n",
        "    cnn_loss.backward()\n",
        "\n",
        "    cnn_optimizer.step()\n",
        "\n",
        "    cnn_total_loss += cnn_loss.item()\n",
        "\n",
        "  print(\"Average Loss per Data Point (CNN):\", cnn_total_loss / ( 60000 // batch_size ) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jNYmAvKnQdL",
        "outputId": "fd097a00-14e4-4a97-aef4-72d24fed0cd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Loss per Data Point (CNN): 0.4564738065518182\n",
            "Average Loss per Data Point (CNN): 0.0983628458504019\n",
            "Average Loss per Data Point (CNN): 0.06006036332712091\n",
            "Average Loss per Data Point (CNN): 0.050112428111505916\n",
            "Average Loss per Data Point (CNN): 0.04772377556896415\n",
            "Average Loss per Data Point (CNN): 0.03795483663421253\n",
            "Average Loss per Data Point (CNN): 0.032978989761965026\n",
            "Average Loss per Data Point (CNN): 0.025611180635490293\n",
            "Average Loss per Data Point (CNN): 0.025306754801743502\n",
            "Average Loss per Data Point (CNN): 0.019449021827814907\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix(cnn_model, x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fhj_1bAznach",
        "outputId": "99c7f866-417c-408d-f79a-a14abf86e612"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[ 968,    0,    3,    2,    0,    1,    5,    0,    1,    0],\n",
              "        [   0, 1129,    1,    1,    1,    0,    2,    0,    1,    0],\n",
              "        [   0,    3, 1017,    6,    0,    0,    1,    4,    1,    0],\n",
              "        [   0,    0,    2, 1005,    0,    2,    0,    1,    0,    0],\n",
              "        [   0,    0,    0,    0,  964,    0,    5,    1,    2,   10],\n",
              "        [   2,    0,    2,   14,    0,  868,    4,    0,    1,    1],\n",
              "        [   8,    2,    1,    1,    1,    1,  944,    0,    0,    0],\n",
              "        [   0,    1,   12,    2,    0,    0,    0, 1005,    3,    5],\n",
              "        [   5,    0,    4,    4,    1,    0,    2,    2,  953,    3],\n",
              "        [   3,    1,    1,    7,    5,    4,    0,    4,    1,  983]],\n",
              "       dtype=int32),\n",
              " 0.9836)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_MNISTNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN_MNISTNetwork, self).__init__()\n",
        "    self.conv_1 = torch.nn.Conv2d( in_channels = 1, out_channels = 20, kernel_size = (3,3), stride = 1, bias = True )\n",
        "    self.linear = torch.nn.Linear( in_features = 26*26*20, out_features = 10, bias = True )\n",
        "\n",
        "  def forward(self, input_tensor):\n",
        "    reshaped = torch.reshape( input_tensor, (-1, 1, 28, 28) )\n",
        "    conv_results = self.conv_1( reshaped )\n",
        "    conv_results = torch.nn.Sigmoid()( conv_results )\n",
        "    flattened = torch.nn.Flatten()( conv_results )\n",
        "    logits = self.linear( flattened )\n",
        "    probabilities = nn.Softmax( dim = 1 )( logits )\n",
        "    return logits, probabilities"
      ],
      "metadata": {
        "id": "b5WmH2LnqZer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model = CNN_MNISTNetwork()\n",
        "\n",
        "for parameter in cnn_model.parameters():\n",
        "  print( parameter.shape )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLOagbCkUqqV",
        "outputId": "003875e5-a7b5-4ca4-b204-4d7f611f5383"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([20, 1, 3, 3])\n",
            "torch.Size([20])\n",
            "torch.Size([10, 13520])\n",
            "torch.Size([10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "26*26*20"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MY3QTqPhU5Oz",
        "outputId": "83b55a4f-64de-45c9-cda6-0a6af2631efe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13520"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "20*1*3*3 + 20 + 10*13520 + 10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C277jhlWUxKA",
        "outputId": "4737256d-4dfe-40f1-95b4-8c0c71fc0a6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "135410"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see from the above that the total number of parameters in this one layer CNN is 135,410 parameters."
      ],
      "metadata": {
        "id": "BCC1ZPoVVVGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MNISTNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MNISTNetwork, self).__init__()\n",
        "\n",
        "    self.layer_1 = torch.nn.Linear( in_features = 784, out_features = 170, bias = True )\n",
        "    self.layer_2 = torch.nn.Linear( in_features = 170, out_features = 10, bias = True)\n",
        "\n",
        "  def forward(self, input_tensor):\n",
        "    flattened = nn.Flatten()( input_tensor )\n",
        "\n",
        "    hidden_nodes = self.layer_1( flattened )\n",
        "    hidden_nodes = torch.nn.Sigmoid()( hidden_nodes )\n",
        "\n",
        "    logits = self.layer_2( hidden_nodes )\n",
        "    final_probabilities = nn.Softmax( dim = 1 )( logits )\n",
        "\n",
        "    return logits, final_probabilities"
      ],
      "metadata": {
        "id": "iZdB3aIDU8Jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MNISTNetwork()\n",
        "\n",
        "for parameter in model.parameters():\n",
        "  print( parameter.shape )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kB74ZTpmVCp5",
        "outputId": "fc06c98e-e304-4e84-f4db-43aedf5ddfe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([170, 784])\n",
            "torch.Size([170])\n",
            "torch.Size([10, 170])\n",
            "torch.Size([10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "170*784 + 170 + 10*170 + 10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKSqXWrrVIlm",
        "outputId": "f2b1d878-acad-47bc-e1b8-23e75293ddb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "135160"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see from this that the total number of parameters in the 1 hidden layer network (with 170 nodes) is 135,160, which is approximately the total number of parameters in the CNN network (hence why I chose 170 as the total number of nodes in the hidden layer). This means that the comparison between the two networks is 'fair' - one doesn't have many more parameters than the other, giving it an advantage in what it can fit. We just see that the convolutional network is better suited to learning the kinds of features that are relevant to this task."
      ],
      "metadata": {
        "id": "ok5PtaClVbnF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uXgcTUynVLgw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}