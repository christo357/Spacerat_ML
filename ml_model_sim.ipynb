{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"results_sims_steps/sims\"\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'filters': [16,32, 64],\n",
    "    'kernel_size': [3,5],\n",
    "    'dropout_rate': [0],\n",
    "    'learning_rate': [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "    'batch_size': [16, 32, 64]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 33\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m input_tensor, label\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mCombinedArrayDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m, in \u001b[0;36mCombinedArrayDataset.__init__\u001b[0;34m(self, folder_path)\u001b[0m\n\u001b[1;32m      6\u001b[0m combined_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(data_frames, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Parse the arrays and labels\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbelief \u001b[38;5;241m=\u001b[39m \u001b[43mcombined_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbelief\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43meval\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mship \u001b[38;5;241m=\u001b[39m combined_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mship\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28meval\u001b[39m(x), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32))\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels \u001b[38;5;241m=\u001b[39m combined_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mremain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m~/PycharmProjects/ML_spacerat/mlrat_venv/lib/python3.12/site-packages/pandas/core/series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/ML_spacerat/mlrat_venv/lib/python3.12/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/ML_spacerat/mlrat_venv/lib/python3.12/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/PycharmProjects/ML_spacerat/mlrat_venv/lib/python3.12/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/ML_spacerat/mlrat_venv/lib/python3.12/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m, in \u001b[0;36mCombinedArrayDataset.__init__.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      6\u001b[0m combined_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(data_frames, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Parse the arrays and labels\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbelief \u001b[38;5;241m=\u001b[39m combined_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbelief\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28;43meval\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32))\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mship \u001b[38;5;241m=\u001b[39m combined_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mship\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28meval\u001b[39m(x), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32))\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels \u001b[38;5;241m=\u001b[39m combined_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mremain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m<string>:0\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class CombinedArrayDataset(Dataset):\n",
    "    def __init__(self, folder_path):\n",
    "        # Load all CSV files in the folder\n",
    "        csv_files = glob(os.path.join(folder_path, \"*.csv\"))\n",
    "        data_frames = [pd.read_csv(file) for file in csv_files]\n",
    "        combined_df = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "        # Parse the arrays and labels\n",
    "        self.belief = combined_df['belief'].apply(lambda x: np.array(eval(x), dtype=np.float32)).tolist()\n",
    "        self.ship = combined_df['ship'].apply(lambda x: np.array(eval(x), dtype=np.float32)).tolist()\n",
    "        self.labels = combined_df['remain'].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the array and label for the given index\n",
    "        belief = self.belief[idx]\n",
    "        ship = self.ship[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        belief = torch.tensor(belief, dtype=torch.float32)  # Ensure float32 for features\n",
    "        ship = torch.tensor(ship, dtype= torch.float32)\n",
    "        label = torch.tensor(label, dtype=torch.float32)  # Ensure long for labels\n",
    "\n",
    "        # Combine belief and ship as two channels\n",
    "        input_tensor = torch.stack((belief, ship), dim=0)  # Shape: (2, height, width)\n",
    "        return input_tensor, label\n",
    "\n",
    "# Example usage\n",
    "\n",
    "dataset = CombinedArrayDataset(data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a PyTorch model class defined as `MyCNNModel`\n",
    "class MyCNNModel(nn.Module):\n",
    "    def __init__(self, filters, kernel_size, strides, dropout_rate):\n",
    "        super(MyCNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(2, filters, kernel_size, stride=strides, padding = 1)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Calculate fc1 input size\n",
    "         # Calculate the size after 2 Conv layers and pooling\n",
    "        # After conv1: (30 - 3 + 2 * 1) / 1 + 1 = 30\n",
    "        # After pool: 30 / 2 = 15\n",
    "        # After conv2: (15 - 3 + 2 * 1) / 1 + 1 = 15\n",
    "        # After pool: 15 / 2 = 7\n",
    "        # conv1_output = (30-filters+2*1)//strides + 1\n",
    "        # after_pool1 = conv1_output//2 + 1\n",
    "        # conv2_output = (after_pool2-filters+2*1)//strides + 1\n",
    "        # after_pool2 = conv2_output//2 + 1\n",
    "        # self.fc_input_size = filters * 2 * 7 * 7  # Updated input size after conv2 and pool\n",
    "        \n",
    "        conv_out_size = ((30 - kernel_size + 2) // strides) + 1  \n",
    "        pool_out_size = ((conv_out_size - 2) // 2) + 1  \n",
    "        fc1_input_size = filters * (pool_out_size ** 2)\n",
    "\n",
    "        self.fc1 = nn.Linear(fc1_input_size, 256)  # Adjust based on input size\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128,1)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.flatten = nn.Flatten()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.dropout(self.relu(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j2/56dmz8kj7h5fyr879140wlzr0000gn/T/ipykernel_72000/1481307314.py:2: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  device = torch.device(\"mps\" if torch.has_mps else \"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: mps\n",
      "Training with parameters: {'batch_size': 16, 'dropout_rate': 0, 'filters': 32, 'kernel_size': 3, 'learning_rate': 0.1, 'strides': 1}\n",
      "Epoch [1/50], Loss: 16194.9471, Test Loss: 3248.9178\n",
      "Epoch [2/50], Loss: 3292.5617, Test Loss: 3300.7354\n",
      "Epoch [3/50], Loss: 3288.8790, Test Loss: 3251.8031\n",
      "Epoch [4/50], Loss: 3286.0268, Test Loss: 3247.1091\n",
      "Epoch [5/50], Loss: 3285.3208, Test Loss: 3251.6914\n",
      "Epoch [6/50], Loss: 3284.8167, Test Loss: 3251.4483\n",
      "Epoch [7/50], Loss: 3285.5276, Test Loss: 3251.2041\n",
      "Epoch [8/50], Loss: 3283.7909, Test Loss: 3247.1965\n",
      "Epoch [9/50], Loss: 3283.5549, Test Loss: 3268.7253\n",
      "Epoch [10/50], Loss: 3285.0224, Test Loss: 3247.1959\n",
      "Epoch [11/50], Loss: 3282.4108, Test Loss: 3270.2702\n",
      "Epoch [12/50], Loss: 3281.5287, Test Loss: 3271.5888\n",
      "Epoch [13/50], Loss: 3283.2140, Test Loss: 3247.9420\n",
      "Epoch [14/50], Loss: 3281.4881, Test Loss: 3247.5062\n",
      "Epoch [15/50], Loss: 3280.3637, Test Loss: 3248.3813\n",
      "Epoch [16/50], Loss: 3278.6056, Test Loss: 3247.3535\n",
      "Epoch [17/50], Loss: 3275.4492, Test Loss: 3249.3094\n",
      "Epoch [18/50], Loss: 3275.0328, Test Loss: 3247.2326\n",
      "Epoch [19/50], Loss: 3275.1366, Test Loss: 3247.9827\n",
      "Epoch [20/50], Loss: 3275.8974, Test Loss: 3247.8735\n",
      "Epoch [21/50], Loss: 3275.3033, Test Loss: 3247.1238\n",
      "Epoch [22/50], Loss: 3275.1655, Test Loss: 3248.8498\n",
      "Epoch [23/50], Loss: 3275.1832, Test Loss: 3247.0996\n",
      "Epoch [24/50], Loss: 3275.4357, Test Loss: 3247.9574\n",
      "Epoch [25/50], Loss: 3275.3932, Test Loss: 3247.8650\n",
      "Epoch [26/50], Loss: 3275.2464, Test Loss: 3247.4258\n",
      "Epoch [27/50], Loss: 3275.1777, Test Loss: 3247.8111\n",
      "Epoch [28/50], Loss: 3275.4425, Test Loss: 3247.1631\n",
      "Epoch [29/50], Loss: 3275.4410, Test Loss: 3247.0630\n",
      "Epoch [30/50], Loss: 3275.1389, Test Loss: 3247.2185\n",
      "Epoch [31/50], Loss: 3275.1697, Test Loss: 3247.3400\n",
      "Epoch [32/50], Loss: 3275.4949, Test Loss: 3250.1297\n",
      "Epoch [33/50], Loss: 3275.1702, Test Loss: 3251.6753\n",
      "Epoch [34/50], Loss: 3275.4271, Test Loss: 3249.9553\n",
      "Epoch [35/50], Loss: 3275.3114, Test Loss: 3248.1309\n",
      "Epoch [36/50], Loss: 3275.2178, Test Loss: 3247.2262\n",
      "Epoch [37/50], Loss: 3275.3286, Test Loss: 3249.8693\n",
      "Epoch [38/50], Loss: 3275.5042, Test Loss: 3247.4138\n",
      "Epoch [39/50], Loss: 3275.6185, Test Loss: 3247.7521\n",
      "Epoch [40/50], Loss: 3275.3624, Test Loss: 3252.3806\n",
      "Epoch [41/50], Loss: 3275.2314, Test Loss: 3247.2345\n",
      "Epoch [42/50], Loss: 3275.6890, Test Loss: 3247.4256\n",
      "Epoch [43/50], Loss: 3275.2787, Test Loss: 3247.1746\n",
      "Epoch [44/50], Loss: 3274.9533, Test Loss: 3248.4815\n",
      "Epoch [45/50], Loss: 3275.4639, Test Loss: 3250.5888\n",
      "Epoch [46/50], Loss: 3275.3151, Test Loss: 3247.1298\n",
      "Epoch [47/50], Loss: 3275.8859, Test Loss: 3247.3003\n",
      "Epoch [48/50], Loss: 3275.2234, Test Loss: 3249.3797\n",
      "Epoch [49/50], Loss: 3275.0382, Test Loss: 3254.3861\n",
      "Epoch [50/50], Loss: 3275.4051, Test Loss: 3251.3731\n",
      "Train Loss: 3275.4051, Test Loss: 3251.3731\n",
      "Training with parameters: {'batch_size': 16, 'dropout_rate': 0, 'filters': 32, 'kernel_size': 3, 'learning_rate': 0.1, 'strides': 2}\n",
      "Epoch [1/50], Loss: 2075.9247, Test Loss: 4907.1914\n",
      "Epoch [2/50], Loss: 3289.8373, Test Loss: 3289.5671\n",
      "Epoch [3/50], Loss: 3287.7572, Test Loss: 3259.1481\n",
      "Epoch [4/50], Loss: 3285.6245, Test Loss: 3265.4850\n",
      "Epoch [5/50], Loss: 3286.0490, Test Loss: 3247.6566\n",
      "Epoch [6/50], Loss: 3285.2200, Test Loss: 3248.1862\n",
      "Epoch [7/50], Loss: 3285.0009, Test Loss: 3247.0852\n",
      "Epoch [8/50], Loss: 3281.7198, Test Loss: 3250.1993\n",
      "Epoch [9/50], Loss: 3282.2276, Test Loss: 3253.7412\n",
      "Epoch [10/50], Loss: 3281.5082, Test Loss: 3266.8732\n",
      "Epoch [11/50], Loss: 3280.7054, Test Loss: 3248.5448\n",
      "Epoch [12/50], Loss: 3278.9035, Test Loss: 3247.0690\n",
      "Epoch [13/50], Loss: 3275.9931, Test Loss: 3249.2862\n",
      "Epoch [14/50], Loss: 3275.2721, Test Loss: 3249.4691\n",
      "Epoch [15/50], Loss: 3275.1275, Test Loss: 3247.6555\n",
      "Epoch [16/50], Loss: 3275.2059, Test Loss: 3247.0737\n",
      "Epoch [17/50], Loss: 3275.1662, Test Loss: 3249.1929\n",
      "Epoch [18/50], Loss: 3275.1462, Test Loss: 3254.0651\n",
      "Epoch [19/50], Loss: 3275.0780, Test Loss: 3247.5361\n",
      "Epoch [20/50], Loss: 3275.4960, Test Loss: 3247.3316\n",
      "Epoch [21/50], Loss: 3275.5088, Test Loss: 3247.6499\n",
      "Epoch [22/50], Loss: 3275.1799, Test Loss: 3247.1674\n",
      "Epoch [23/50], Loss: 3275.2785, Test Loss: 3247.6448\n",
      "Epoch [24/50], Loss: 3275.7635, Test Loss: 3250.4613\n",
      "Epoch [25/50], Loss: 3275.0280, Test Loss: 3248.3759\n",
      "Epoch [26/50], Loss: 3275.3151, Test Loss: 3247.3808\n",
      "Epoch [27/50], Loss: 3275.2259, Test Loss: 3247.7689\n",
      "Epoch [28/50], Loss: 3275.3609, Test Loss: 3249.3197\n",
      "Epoch [29/50], Loss: 3275.1201, Test Loss: 3247.0901\n",
      "Epoch [30/50], Loss: 3275.5303, Test Loss: 3247.0846\n",
      "Epoch [31/50], Loss: 3275.2957, Test Loss: 3247.0996\n",
      "Epoch [32/50], Loss: 3275.5040, Test Loss: 3247.2512\n",
      "Epoch [33/50], Loss: 3274.9872, Test Loss: 3248.8308\n",
      "Epoch [34/50], Loss: 3275.4177, Test Loss: 3248.0646\n",
      "Epoch [35/50], Loss: 3275.0490, Test Loss: 3247.1121\n",
      "Epoch [36/50], Loss: 3274.8777, Test Loss: 3247.0819\n",
      "Epoch [37/50], Loss: 3275.4523, Test Loss: 3247.2185\n",
      "Epoch [38/50], Loss: 3275.2772, Test Loss: 3247.0644\n",
      "Epoch [39/50], Loss: 3275.3855, Test Loss: 3247.7748\n",
      "Epoch [40/50], Loss: 3275.3581, Test Loss: 3248.4457\n",
      "Epoch [41/50], Loss: 3275.6585, Test Loss: 3247.5525\n",
      "Epoch [42/50], Loss: 3275.2067, Test Loss: 3249.3164\n",
      "Epoch [43/50], Loss: 3275.1431, Test Loss: 3254.2051\n",
      "Epoch [44/50], Loss: 3275.3908, Test Loss: 3247.0811\n",
      "Epoch [45/50], Loss: 3275.2588, Test Loss: 3247.2676\n",
      "Epoch [46/50], Loss: 3275.3776, Test Loss: 3247.9482\n",
      "Epoch [47/50], Loss: 3275.4391, Test Loss: 3247.1909\n",
      "Epoch [48/50], Loss: 3275.6230, Test Loss: 3248.9076\n",
      "Epoch [49/50], Loss: 3275.3208, Test Loss: 3247.3622\n",
      "Epoch [50/50], Loss: 3275.4967, Test Loss: 3249.4721\n",
      "Train Loss: 3275.4967, Test Loss: 3249.4721\n",
      "Training with parameters: {'batch_size': 16, 'dropout_rate': 0, 'filters': 32, 'kernel_size': 3, 'learning_rate': 0.01, 'strides': 1}\n",
      "Epoch [1/50], Loss: 1826.6865, Test Loss: 1665.6938\n",
      "Epoch [2/50], Loss: 1601.9817, Test Loss: 1532.8986\n",
      "Epoch [3/50], Loss: 1521.8903, Test Loss: 1451.9237\n",
      "Epoch [4/50], Loss: 1467.7718, Test Loss: 1401.0391\n",
      "Epoch [5/50], Loss: 1424.4179, Test Loss: 1427.7023\n",
      "Epoch [6/50], Loss: 1389.8201, Test Loss: 1365.7581\n",
      "Epoch [7/50], Loss: 1344.0141, Test Loss: 1332.0273\n",
      "Epoch [8/50], Loss: 1315.3141, Test Loss: 1348.7900\n",
      "Epoch [9/50], Loss: 1282.1324, Test Loss: 1283.5124\n",
      "Epoch [10/50], Loss: 1256.4987, Test Loss: 1268.2808\n",
      "Epoch [11/50], Loss: 1232.6298, Test Loss: 1264.2585\n",
      "Epoch [12/50], Loss: 1216.0988, Test Loss: 1230.2598\n",
      "Epoch [13/50], Loss: 1189.3958, Test Loss: 1172.3165\n",
      "Epoch [14/50], Loss: 1173.2069, Test Loss: 1190.6687\n",
      "Epoch [15/50], Loss: 1154.6057, Test Loss: 1158.6194\n",
      "Epoch [16/50], Loss: 1143.1442, Test Loss: 1143.4027\n",
      "Epoch [17/50], Loss: 1125.5742, Test Loss: 1208.2643\n",
      "Epoch [18/50], Loss: 1106.8867, Test Loss: 1112.7831\n",
      "Epoch [19/50], Loss: 1097.8777, Test Loss: 1092.5901\n",
      "Epoch [20/50], Loss: 1086.1348, Test Loss: 1107.8288\n",
      "Epoch [21/50], Loss: 1076.2852, Test Loss: 1154.7677\n",
      "Epoch [22/50], Loss: 1062.7344, Test Loss: 1163.5988\n",
      "Epoch [23/50], Loss: 1057.0119, Test Loss: 1078.0896\n",
      "Epoch [24/50], Loss: 1042.1608, Test Loss: 1074.7042\n",
      "Epoch [25/50], Loss: 1029.0214, Test Loss: 1096.1566\n",
      "Epoch [26/50], Loss: 1024.1113, Test Loss: 1062.1417\n",
      "Epoch [27/50], Loss: 1014.0947, Test Loss: 1213.0463\n",
      "Epoch [28/50], Loss: 1011.3299, Test Loss: 1009.1985\n",
      "Epoch [29/50], Loss: 1005.1804, Test Loss: 1018.2212\n",
      "Epoch [30/50], Loss: 995.7793, Test Loss: 1104.1481\n",
      "Epoch [31/50], Loss: 989.0493, Test Loss: 1001.4844\n",
      "Epoch [32/50], Loss: 981.9824, Test Loss: 1048.7822\n",
      "Epoch [33/50], Loss: 974.9929, Test Loss: 1021.9273\n",
      "Epoch [34/50], Loss: 971.2472, Test Loss: 1028.3008\n",
      "Epoch [35/50], Loss: 968.5240, Test Loss: 989.7915\n",
      "Epoch [36/50], Loss: 958.9630, Test Loss: 987.9613\n",
      "Epoch [37/50], Loss: 958.1482, Test Loss: 1037.4655\n",
      "Epoch [38/50], Loss: 950.7459, Test Loss: 1009.8121\n",
      "Epoch [39/50], Loss: 945.9632, Test Loss: 983.9998\n",
      "Epoch [40/50], Loss: 941.4611, Test Loss: 1023.2699\n",
      "Epoch [41/50], Loss: 941.9450, Test Loss: 960.0794\n",
      "Epoch [42/50], Loss: 925.1305, Test Loss: 1012.1407\n",
      "Epoch [43/50], Loss: 925.1585, Test Loss: 970.6451\n",
      "Epoch [44/50], Loss: 922.1413, Test Loss: 965.1509\n",
      "Epoch [45/50], Loss: 920.3730, Test Loss: 939.2447\n",
      "Epoch [46/50], Loss: 913.8744, Test Loss: 936.8376\n",
      "Epoch [47/50], Loss: 909.8170, Test Loss: 959.1540\n",
      "Epoch [48/50], Loss: 903.6094, Test Loss: 948.6044\n",
      "Epoch [49/50], Loss: 900.4535, Test Loss: 947.7778\n",
      "Epoch [50/50], Loss: 899.8562, Test Loss: 963.7733\n",
      "Train Loss: 899.8562, Test Loss: 963.7733\n",
      "Training with parameters: {'batch_size': 16, 'dropout_rate': 0, 'filters': 32, 'kernel_size': 3, 'learning_rate': 0.01, 'strides': 2}\n",
      "Epoch [1/50], Loss: 1818.9106, Test Loss: 1651.4661\n",
      "Epoch [2/50], Loss: 1661.0004, Test Loss: 1591.1902\n",
      "Epoch [3/50], Loss: 1610.1398, Test Loss: 1594.1060\n",
      "Epoch [4/50], Loss: 1567.9965, Test Loss: 1519.2041\n",
      "Epoch [5/50], Loss: 1531.4236, Test Loss: 1481.6661\n",
      "Epoch [6/50], Loss: 1501.2640, Test Loss: 1468.6717\n",
      "Epoch [7/50], Loss: 1475.8791, Test Loss: 1485.3201\n",
      "Epoch [8/50], Loss: 1441.4102, Test Loss: 1411.6460\n",
      "Epoch [9/50], Loss: 1411.1287, Test Loss: 1376.7463\n",
      "Epoch [10/50], Loss: 1390.0017, Test Loss: 1595.4867\n",
      "Epoch [11/50], Loss: 1364.3250, Test Loss: 1371.4630\n",
      "Epoch [12/50], Loss: 1348.2045, Test Loss: 1337.7772\n",
      "Epoch [13/50], Loss: 1328.2280, Test Loss: 1404.9819\n",
      "Epoch [14/50], Loss: 1303.7039, Test Loss: 1321.6898\n",
      "Epoch [15/50], Loss: 1286.8202, Test Loss: 1292.0260\n",
      "Epoch [16/50], Loss: 1270.1150, Test Loss: 1302.5217\n",
      "Epoch [17/50], Loss: 1246.1710, Test Loss: 1289.0882\n",
      "Epoch [18/50], Loss: 1228.1387, Test Loss: 1211.5099\n",
      "Epoch [19/50], Loss: 1220.6355, Test Loss: 1299.4067\n",
      "Epoch [20/50], Loss: 1206.2210, Test Loss: 1196.8112\n",
      "Epoch [21/50], Loss: 1190.8226, Test Loss: 1184.7988\n",
      "Epoch [22/50], Loss: 1171.8920, Test Loss: 1187.2491\n",
      "Epoch [23/50], Loss: 1176.3413, Test Loss: 1355.8248\n",
      "Epoch [24/50], Loss: 1159.6099, Test Loss: 1166.6447\n",
      "Epoch [25/50], Loss: 1149.4279, Test Loss: 1235.2151\n",
      "Epoch [26/50], Loss: 1139.8305, Test Loss: 1136.5235\n",
      "Epoch [27/50], Loss: 1129.3832, Test Loss: 1168.9913\n",
      "Epoch [28/50], Loss: 1121.1852, Test Loss: 1479.3786\n",
      "Epoch [29/50], Loss: 1113.4397, Test Loss: 1121.0090\n",
      "Epoch [30/50], Loss: 1105.3665, Test Loss: 1116.1581\n",
      "Epoch [31/50], Loss: 1106.2287, Test Loss: 1138.7240\n",
      "Epoch [32/50], Loss: 1090.9873, Test Loss: 1114.1203\n",
      "Epoch [33/50], Loss: 1086.3977, Test Loss: 1091.0417\n",
      "Epoch [34/50], Loss: 1073.6491, Test Loss: 1100.4545\n",
      "Epoch [35/50], Loss: 1076.8187, Test Loss: 1105.4059\n",
      "Epoch [36/50], Loss: 1064.8187, Test Loss: 1226.9173\n",
      "Epoch [37/50], Loss: 1056.9071, Test Loss: 1065.9818\n",
      "Epoch [38/50], Loss: 1052.3996, Test Loss: 1144.8304\n",
      "Epoch [39/50], Loss: 1051.0002, Test Loss: 1044.7124\n",
      "Epoch [40/50], Loss: 1040.8879, Test Loss: 1141.6939\n",
      "Epoch [41/50], Loss: 1038.8101, Test Loss: 1190.3113\n",
      "Epoch [42/50], Loss: 1027.2328, Test Loss: 1077.9567\n",
      "Epoch [43/50], Loss: 1030.8588, Test Loss: 1069.2885\n",
      "Epoch [44/50], Loss: 1025.1939, Test Loss: 1066.7816\n",
      "Epoch [45/50], Loss: 1024.9648, Test Loss: 1046.2272\n",
      "Epoch [46/50], Loss: 1008.2241, Test Loss: 1064.4080\n",
      "Epoch [47/50], Loss: 1014.4090, Test Loss: 1046.8198\n",
      "Epoch [48/50], Loss: 1001.4288, Test Loss: 1089.5719\n",
      "Epoch [49/50], Loss: 1001.2376, Test Loss: 1130.5907\n",
      "Epoch [50/50], Loss: 1003.0874, Test Loss: 1030.7127\n",
      "Train Loss: 1003.0874, Test Loss: 1030.7127\n",
      "Training with parameters: {'batch_size': 16, 'dropout_rate': 0, 'filters': 32, 'kernel_size': 3, 'learning_rate': 0.001, 'strides': 1}\n",
      "Epoch [1/50], Loss: 1929.7943, Test Loss: 1643.7689\n",
      "Epoch [2/50], Loss: 1640.7204, Test Loss: 1545.8590\n",
      "Epoch [3/50], Loss: 1546.2343, Test Loss: 1484.6102\n",
      "Epoch [4/50], Loss: 1470.6486, Test Loss: 1406.5956\n",
      "Epoch [5/50], Loss: 1396.6069, Test Loss: 1370.1329\n",
      "Epoch [6/50], Loss: 1319.1507, Test Loss: 1275.4966\n",
      "Epoch [7/50], Loss: 1240.9067, Test Loss: 1290.5999\n",
      "Epoch [8/50], Loss: 1171.4853, Test Loss: 1135.6977\n",
      "Epoch [9/50], Loss: 1107.1685, Test Loss: 1095.5175\n",
      "Epoch [10/50], Loss: 1051.4218, Test Loss: 1092.7760\n",
      "Epoch [11/50], Loss: 1007.5656, Test Loss: 1153.8612\n",
      "Epoch [12/50], Loss: 962.4843, Test Loss: 961.4920\n",
      "Epoch [13/50], Loss: 927.2728, Test Loss: 948.0071\n",
      "Epoch [14/50], Loss: 897.1080, Test Loss: 925.2198\n",
      "Epoch [15/50], Loss: 866.9397, Test Loss: 909.5786\n",
      "Epoch [16/50], Loss: 839.3848, Test Loss: 864.1258\n",
      "Epoch [17/50], Loss: 817.4016, Test Loss: 851.8168\n",
      "Epoch [18/50], Loss: 798.2517, Test Loss: 844.3068\n",
      "Epoch [19/50], Loss: 780.5893, Test Loss: 823.9363\n",
      "Epoch [20/50], Loss: 758.4712, Test Loss: 785.4015\n",
      "Epoch [21/50], Loss: 739.6516, Test Loss: 804.2302\n",
      "Epoch [22/50], Loss: 725.4758, Test Loss: 780.8502\n",
      "Epoch [23/50], Loss: 710.0589, Test Loss: 765.2055\n",
      "Epoch [24/50], Loss: 694.8778, Test Loss: 743.5531\n",
      "Epoch [25/50], Loss: 683.7629, Test Loss: 728.3343\n",
      "Epoch [26/50], Loss: 674.0641, Test Loss: 746.7839\n",
      "Epoch [27/50], Loss: 663.2248, Test Loss: 704.1161\n",
      "Epoch [28/50], Loss: 648.7655, Test Loss: 690.8960\n",
      "Epoch [29/50], Loss: 637.7734, Test Loss: 729.2355\n",
      "Epoch [30/50], Loss: 632.1620, Test Loss: 695.3953\n",
      "Epoch [31/50], Loss: 620.6041, Test Loss: 675.5935\n",
      "Epoch [32/50], Loss: 612.2657, Test Loss: 662.1633\n",
      "Epoch [33/50], Loss: 601.1537, Test Loss: 645.3223\n",
      "Epoch [34/50], Loss: 595.4722, Test Loss: 662.9704\n",
      "Epoch [35/50], Loss: 585.7108, Test Loss: 652.0734\n",
      "Epoch [36/50], Loss: 578.3578, Test Loss: 632.7699\n",
      "Epoch [37/50], Loss: 572.1322, Test Loss: 706.5576\n",
      "Epoch [38/50], Loss: 564.6559, Test Loss: 631.3808\n",
      "Epoch [39/50], Loss: 560.0133, Test Loss: 635.3671\n",
      "Epoch [40/50], Loss: 552.0483, Test Loss: 616.9508\n",
      "Epoch [41/50], Loss: 546.6960, Test Loss: 631.2388\n",
      "Epoch [42/50], Loss: 541.0814, Test Loss: 597.0236\n",
      "Epoch [43/50], Loss: 534.9202, Test Loss: 599.0970\n",
      "Epoch [44/50], Loss: 530.1590, Test Loss: 591.7070\n",
      "Epoch [45/50], Loss: 526.2350, Test Loss: 595.4982\n",
      "Epoch [46/50], Loss: 516.2132, Test Loss: 575.5503\n",
      "Epoch [47/50], Loss: 513.1381, Test Loss: 577.6999\n",
      "Epoch [48/50], Loss: 508.3268, Test Loss: 576.0007\n",
      "Epoch [49/50], Loss: 506.3760, Test Loss: 570.4106\n",
      "Epoch [50/50], Loss: 500.1502, Test Loss: 558.7648\n",
      "Train Loss: 500.1502, Test Loss: 558.7648\n",
      "Training with parameters: {'batch_size': 16, 'dropout_rate': 0, 'filters': 32, 'kernel_size': 3, 'learning_rate': 0.001, 'strides': 2}\n",
      "Epoch [1/50], Loss: 1983.8946, Test Loss: 1700.2947\n",
      "Epoch [2/50], Loss: 1661.2308, Test Loss: 1592.5111\n",
      "Epoch [3/50], Loss: 1601.6934, Test Loss: 1547.1275\n",
      "Epoch [4/50], Loss: 1556.5379, Test Loss: 1542.9015\n",
      "Epoch [5/50], Loss: 1515.2420, Test Loss: 1481.0613\n",
      "Epoch [6/50], Loss: 1467.5222, Test Loss: 1433.7261\n",
      "Epoch [7/50], Loss: 1418.3317, Test Loss: 1384.9803\n",
      "Epoch [8/50], Loss: 1364.3949, Test Loss: 1315.8778\n",
      "Epoch [9/50], Loss: 1312.0265, Test Loss: 1336.0795\n",
      "Epoch [10/50], Loss: 1258.4119, Test Loss: 1247.6159\n",
      "Epoch [11/50], Loss: 1204.7383, Test Loss: 1184.8276\n",
      "Epoch [12/50], Loss: 1149.9697, Test Loss: 1175.1332\n",
      "Epoch [13/50], Loss: 1104.6148, Test Loss: 1173.1305\n",
      "Epoch [14/50], Loss: 1062.8512, Test Loss: 1086.5772\n",
      "Epoch [15/50], Loss: 1026.1588, Test Loss: 1044.1651\n",
      "Epoch [16/50], Loss: 992.8153, Test Loss: 1062.1493\n",
      "Epoch [17/50], Loss: 959.6108, Test Loss: 1017.7270\n",
      "Epoch [18/50], Loss: 930.9897, Test Loss: 990.1801\n",
      "Epoch [19/50], Loss: 905.5717, Test Loss: 925.5895\n",
      "Epoch [20/50], Loss: 887.2030, Test Loss: 923.6175\n",
      "Epoch [21/50], Loss: 863.6616, Test Loss: 900.9596\n",
      "Epoch [22/50], Loss: 844.4930, Test Loss: 1013.4041\n",
      "Epoch [23/50], Loss: 830.7665, Test Loss: 871.1931\n",
      "Epoch [24/50], Loss: 809.0772, Test Loss: 862.7676\n",
      "Epoch [25/50], Loss: 793.2651, Test Loss: 880.4550\n",
      "Epoch [26/50], Loss: 780.5518, Test Loss: 813.0088\n",
      "Epoch [27/50], Loss: 768.5297, Test Loss: 828.7480\n",
      "Epoch [28/50], Loss: 757.0741, Test Loss: 813.7352\n",
      "Epoch [29/50], Loss: 746.4529, Test Loss: 814.3832\n",
      "Epoch [30/50], Loss: 730.0077, Test Loss: 820.5620\n",
      "Epoch [31/50], Loss: 725.3379, Test Loss: 824.3661\n",
      "Epoch [32/50], Loss: 714.6470, Test Loss: 756.3934\n",
      "Epoch [33/50], Loss: 703.1216, Test Loss: 750.9257\n",
      "Epoch [34/50], Loss: 695.5562, Test Loss: 785.1800\n",
      "Epoch [35/50], Loss: 689.2197, Test Loss: 757.7193\n",
      "Epoch [36/50], Loss: 675.5516, Test Loss: 739.8051\n",
      "Epoch [37/50], Loss: 669.8393, Test Loss: 729.6837\n",
      "Epoch [38/50], Loss: 662.6939, Test Loss: 733.7463\n",
      "Epoch [39/50], Loss: 656.5024, Test Loss: 755.2408\n",
      "Epoch [40/50], Loss: 651.8078, Test Loss: 769.7098\n",
      "Epoch [41/50], Loss: 643.4717, Test Loss: 732.3138\n",
      "Epoch [42/50], Loss: 636.6066, Test Loss: 697.7723\n",
      "Epoch [43/50], Loss: 629.2725, Test Loss: 681.3944\n",
      "Epoch [44/50], Loss: 624.6615, Test Loss: 692.7541\n",
      "Epoch [45/50], Loss: 613.8071, Test Loss: 680.8571\n",
      "Epoch [46/50], Loss: 612.0020, Test Loss: 686.7630\n",
      "Epoch [47/50], Loss: 603.7262, Test Loss: 667.6853\n",
      "Epoch [48/50], Loss: 601.3205, Test Loss: 698.4759\n",
      "Epoch [49/50], Loss: 595.2994, Test Loss: 675.5295\n",
      "Epoch [50/50], Loss: 589.4568, Test Loss: 661.1930\n",
      "Train Loss: 589.4568, Test Loss: 661.1930\n",
      "Training with parameters: {'batch_size': 16, 'dropout_rate': 0, 'filters': 32, 'kernel_size': 3, 'learning_rate': 0.0001, 'strides': 1}\n",
      "Epoch [1/50], Loss: 2927.9568, Test Loss: 1847.8998\n",
      "Epoch [2/50], Loss: 1812.3678, Test Loss: 1745.3741\n",
      "Epoch [3/50], Loss: 1768.1721, Test Loss: 1748.8880\n",
      "Epoch [4/50], Loss: 1739.0794, Test Loss: 1723.2955\n",
      "Epoch [5/50], Loss: 1711.6339, Test Loss: 1668.5999\n",
      "Epoch [6/50], Loss: 1685.5079, Test Loss: 1646.6549\n",
      "Epoch [7/50], Loss: 1659.5410, Test Loss: 1631.1854\n",
      "Epoch [8/50], Loss: 1642.1352, Test Loss: 1609.0794\n",
      "Epoch [9/50], Loss: 1624.0152, Test Loss: 1613.2192\n",
      "Epoch [10/50], Loss: 1609.9788, Test Loss: 1606.8631\n",
      "Epoch [11/50], Loss: 1595.0503, Test Loss: 1564.1120\n",
      "Epoch [12/50], Loss: 1583.9781, Test Loss: 1566.8658\n",
      "Epoch [13/50], Loss: 1570.6735, Test Loss: 1639.1951\n",
      "Epoch [14/50], Loss: 1558.8990, Test Loss: 1529.2444\n",
      "Epoch [15/50], Loss: 1548.0935, Test Loss: 1519.8285\n",
      "Epoch [16/50], Loss: 1535.3041, Test Loss: 1536.3562\n",
      "Epoch [17/50], Loss: 1525.3336, Test Loss: 1515.8152\n",
      "Epoch [18/50], Loss: 1511.8717, Test Loss: 1490.3620\n",
      "Epoch [19/50], Loss: 1502.2682, Test Loss: 1476.8864\n",
      "Epoch [20/50], Loss: 1491.5644, Test Loss: 1483.7749\n",
      "Epoch [21/50], Loss: 1478.0074, Test Loss: 1455.6461\n",
      "Epoch [22/50], Loss: 1465.1189, Test Loss: 1451.8744\n",
      "Epoch [23/50], Loss: 1455.0224, Test Loss: 1436.2197\n",
      "Epoch [24/50], Loss: 1438.8963, Test Loss: 1434.1029\n",
      "Epoch [25/50], Loss: 1429.5451, Test Loss: 1432.5256\n",
      "Epoch [26/50], Loss: 1417.7370, Test Loss: 1413.1328\n",
      "Epoch [27/50], Loss: 1404.5611, Test Loss: 1392.1615\n",
      "Epoch [28/50], Loss: 1391.7942, Test Loss: 1398.6154\n",
      "Epoch [29/50], Loss: 1378.8725, Test Loss: 1370.4413\n",
      "Epoch [30/50], Loss: 1368.6517, Test Loss: 1362.0675\n",
      "Epoch [31/50], Loss: 1351.6624, Test Loss: 1351.7924\n",
      "Epoch [32/50], Loss: 1340.6567, Test Loss: 1328.7648\n",
      "Epoch [33/50], Loss: 1326.9868, Test Loss: 1444.5846\n",
      "Epoch [34/50], Loss: 1315.3751, Test Loss: 1306.1492\n",
      "Epoch [35/50], Loss: 1300.7207, Test Loss: 1294.0877\n",
      "Epoch [36/50], Loss: 1288.9424, Test Loss: 1289.8613\n",
      "Epoch [37/50], Loss: 1272.1358, Test Loss: 1329.0281\n",
      "Epoch [38/50], Loss: 1258.6656, Test Loss: 1257.8607\n",
      "Epoch [39/50], Loss: 1246.1743, Test Loss: 1252.6091\n",
      "Epoch [40/50], Loss: 1233.6469, Test Loss: 1237.4210\n",
      "Epoch [41/50], Loss: 1217.7819, Test Loss: 1227.7080\n",
      "Epoch [42/50], Loss: 1203.8729, Test Loss: 1223.5084\n",
      "Epoch [43/50], Loss: 1192.8652, Test Loss: 1223.7410\n",
      "Epoch [44/50], Loss: 1178.0973, Test Loss: 1187.2327\n",
      "Epoch [45/50], Loss: 1165.5997, Test Loss: 1185.3166\n",
      "Epoch [46/50], Loss: 1148.7131, Test Loss: 1163.8628\n",
      "Epoch [47/50], Loss: 1134.1841, Test Loss: 1159.3650\n",
      "Epoch [48/50], Loss: 1123.1317, Test Loss: 1135.9284\n",
      "Epoch [49/50], Loss: 1106.8115, Test Loss: 1121.3313\n",
      "Epoch [50/50], Loss: 1093.8743, Test Loss: 1114.8686\n",
      "Train Loss: 1093.8743, Test Loss: 1114.8686\n",
      "Training with parameters: {'batch_size': 16, 'dropout_rate': 0, 'filters': 32, 'kernel_size': 3, 'learning_rate': 0.0001, 'strides': 2}\n",
      "Epoch [1/50], Loss: 3299.7919, Test Loss: 2918.9583\n",
      "Epoch [2/50], Loss: 2034.5481, Test Loss: 1758.8208\n",
      "Epoch [3/50], Loss: 1760.1752, Test Loss: 1739.4363\n",
      "Epoch [4/50], Loss: 1736.6844, Test Loss: 1729.2350\n",
      "Epoch [5/50], Loss: 1720.4699, Test Loss: 1683.8613\n",
      "Epoch [6/50], Loss: 1704.4607, Test Loss: 1683.5031\n",
      "Epoch [7/50], Loss: 1695.0618, Test Loss: 1660.6209\n",
      "Epoch [8/50], Loss: 1682.5880, Test Loss: 1648.4502\n",
      "Epoch [9/50], Loss: 1671.3120, Test Loss: 1644.8829\n",
      "Epoch [10/50], Loss: 1657.8097, Test Loss: 1643.8398\n",
      "Epoch [11/50], Loss: 1650.8963, Test Loss: 1628.1474\n",
      "Epoch [12/50], Loss: 1639.5397, Test Loss: 1633.6855\n",
      "Epoch [13/50], Loss: 1629.2763, Test Loss: 1621.7081\n",
      "Epoch [14/50], Loss: 1620.5900, Test Loss: 1598.0310\n",
      "Epoch [15/50], Loss: 1611.9565, Test Loss: 1611.5120\n",
      "Epoch [16/50], Loss: 1601.5717, Test Loss: 1584.5594\n",
      "Epoch [17/50], Loss: 1595.6940, Test Loss: 1570.1543\n",
      "Epoch [18/50], Loss: 1589.0742, Test Loss: 1571.4369\n",
      "Epoch [19/50], Loss: 1581.5812, Test Loss: 1616.0971\n",
      "Epoch [20/50], Loss: 1574.6431, Test Loss: 1560.8251\n",
      "Epoch [21/50], Loss: 1568.9964, Test Loss: 1545.5283\n",
      "Epoch [22/50], Loss: 1560.6374, Test Loss: 1539.1395\n",
      "Epoch [23/50], Loss: 1553.5381, Test Loss: 1541.3778\n",
      "Epoch [24/50], Loss: 1545.9000, Test Loss: 1729.3097\n",
      "Epoch [25/50], Loss: 1542.6026, Test Loss: 1525.2474\n",
      "Epoch [26/50], Loss: 1534.5497, Test Loss: 1523.8730\n",
      "Epoch [27/50], Loss: 1531.2832, Test Loss: 1507.4582\n",
      "Epoch [28/50], Loss: 1523.3692, Test Loss: 1530.4197\n",
      "Epoch [29/50], Loss: 1518.1913, Test Loss: 1505.4391\n",
      "Epoch [30/50], Loss: 1509.7327, Test Loss: 1550.6211\n",
      "Epoch [31/50], Loss: 1503.3704, Test Loss: 1492.1095\n",
      "Epoch [32/50], Loss: 1497.2384, Test Loss: 1482.6799\n",
      "Epoch [33/50], Loss: 1491.9212, Test Loss: 1493.3516\n",
      "Epoch [34/50], Loss: 1482.7033, Test Loss: 1468.0981\n",
      "Epoch [35/50], Loss: 1475.2514, Test Loss: 1474.3190\n",
      "Epoch [36/50], Loss: 1467.9918, Test Loss: 1463.7917\n",
      "Epoch [37/50], Loss: 1462.0962, Test Loss: 1452.7708\n",
      "Epoch [38/50], Loss: 1455.5421, Test Loss: 1467.6871\n",
      "Epoch [39/50], Loss: 1448.5402, Test Loss: 1440.1176\n",
      "Epoch [40/50], Loss: 1440.8135, Test Loss: 1437.4890\n",
      "Epoch [41/50], Loss: 1435.2969, Test Loss: 1438.2768\n",
      "Epoch [42/50], Loss: 1426.7626, Test Loss: 1426.1633\n",
      "Epoch [43/50], Loss: 1421.1161, Test Loss: 1431.7231\n",
      "Epoch [44/50], Loss: 1410.9060, Test Loss: 1404.9800\n",
      "Epoch [45/50], Loss: 1406.1580, Test Loss: 1433.9051\n",
      "Epoch [46/50], Loss: 1400.4538, Test Loss: 1399.2520\n",
      "Epoch [47/50], Loss: 1392.9144, Test Loss: 1428.2287\n",
      "Epoch [48/50], Loss: 1385.9174, Test Loss: 1389.5547\n",
      "Epoch [49/50], Loss: 1379.2109, Test Loss: 1379.8197\n",
      "Epoch [50/50], Loss: 1372.3356, Test Loss: 1369.0989\n",
      "Train Loss: 1372.3356, Test Loss: 1369.0989\n",
      "Training with parameters: {'batch_size': 16, 'dropout_rate': 0, 'filters': 32, 'kernel_size': 5, 'learning_rate': 0.1, 'strides': 1}\n",
      "Epoch [1/50], Loss: 24628.4837, Test Loss: 3247.2531\n",
      "Epoch [2/50], Loss: 3292.0955, Test Loss: 3247.1140\n",
      "Epoch [3/50], Loss: 3290.0173, Test Loss: 3255.7740\n",
      "Epoch [4/50], Loss: 3286.6024, Test Loss: 3248.5652\n",
      "Epoch [5/50], Loss: 3285.0595, Test Loss: 3293.2017\n",
      "Epoch [6/50], Loss: 3285.5755, Test Loss: 3250.1450\n",
      "Epoch [7/50], Loss: 3283.6868, Test Loss: 3247.1649\n",
      "Epoch [8/50], Loss: 3284.3359, Test Loss: 3247.0686\n",
      "Epoch [9/50], Loss: 3284.5945, Test Loss: 3310.1640\n",
      "Epoch [10/50], Loss: 3283.2327, Test Loss: 3257.8680\n",
      "Epoch [11/50], Loss: 3284.2320, Test Loss: 3256.6730\n",
      "Epoch [12/50], Loss: 3283.1705, Test Loss: 3249.1932\n",
      "Epoch [13/50], Loss: 3282.6376, Test Loss: 3258.9601\n",
      "Epoch [14/50], Loss: 3283.0078, Test Loss: 3252.6732\n",
      "Epoch [15/50], Loss: 3281.6252, Test Loss: 3253.2225\n",
      "Epoch [16/50], Loss: 3279.0611, Test Loss: 3247.9830\n",
      "Epoch [17/50], Loss: 3275.8704, Test Loss: 3247.2535\n",
      "Epoch [18/50], Loss: 3275.3657, Test Loss: 3249.3209\n",
      "Epoch [19/50], Loss: 3274.9737, Test Loss: 3247.2906\n",
      "Epoch [20/50], Loss: 3275.4348, Test Loss: 3249.8052\n",
      "Epoch [21/50], Loss: 3275.4941, Test Loss: 3247.7317\n",
      "Epoch [22/50], Loss: 3275.3272, Test Loss: 3247.0674\n",
      "Epoch [23/50], Loss: 3275.0819, Test Loss: 3248.1255\n",
      "Epoch [24/50], Loss: 3275.3107, Test Loss: 3248.3450\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39msqueeze(), labels)\n\u001b[1;32m     39\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 40\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     42\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
      "File \u001b[0;32m~/PycharmProjects/ML_spacerat/mlrat_venv/lib/python3.12/site-packages/torch/optim/optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    485\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/PycharmProjects/ML_spacerat/mlrat_venv/lib/python3.12/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/PycharmProjects/ML_spacerat/mlrat_venv/lib/python3.12/site-packages/torch/optim/adam.py:223\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    211\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    213\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    214\u001b[0m         group,\n\u001b[1;32m    215\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    220\u001b[0m         state_steps,\n\u001b[1;32m    221\u001b[0m     )\n\u001b[0;32m--> 223\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/PycharmProjects/ML_spacerat/mlrat_venv/lib/python3.12/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/ML_spacerat/mlrat_venv/lib/python3.12/site-packages/torch/optim/adam.py:784\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    782\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 784\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/ML_spacerat/mlrat_venv/lib/python3.12/site-packages/torch/optim/adam.py:430\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    428\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 430\u001b[0m         denom \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    432\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"mps\" if torch.has_mps else \"cpu\")\n",
    "print(f\"using device: {device}\")\n",
    "# Placeholder for results\n",
    "results = []\n",
    "\n",
    "# Dataset (assuming `train_dataset` and `test_dataset` are already defined)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    print(f\"Training with parameters: {params}\")\n",
    "    train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=params['batch_size'], shuffle=False)\n",
    "\n",
    "    # Initialize model\n",
    "    model = MyCNNModel(params['filters'], params['kernel_size'],params['strides'] ,params['dropout_rate']).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "\n",
    "    # Train model\n",
    "    num_epochs = 25\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():  # No need to compute gradients during testing\n",
    "            for inputs, targets in test_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                # Compute the loss\n",
    "                loss = criterion(outputs.squeeze(), targets)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "        avg_test_loss = test_loss / len(test_loader)\n",
    "        test_losses.append(avg_test_loss)\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}, Test Loss: {test_loss / len(test_loader):.4f}\")\n",
    "\n",
    "    # Evaluate model on validation data\n",
    "    # model.eval()\n",
    "    # correct, total = 0, 0\n",
    "    # with torch.no_grad():\n",
    "    #     for images, labels in val_loader:\n",
    "    #         images, labels = images.to(device), labels.to(device)\n",
    "    #         outputs = model(images)\n",
    "    #         _, predicted = torch.max(outputs, 1)\n",
    "    #         total += labels.size(0)\n",
    "    #         correct += (predicted == labels).sum().item()\n",
    "    # val_accuracy = correct / total\n",
    "\n",
    "    # Test model\n",
    "    test_loss, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            # _, predicted = torch.max(outputs, 1)\n",
    "            # total += labels.size(0)\n",
    "            # correct += (predicted == labels).sum().item()\n",
    "    # test_accuracy = correct / total\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "        avg_test_loss = test_loss / len(test_loader)\n",
    "        # test_losses.append(avg_test_loss)\n",
    "    # Save results\n",
    "    results.append((params, avg_train_loss,  avg_test_loss))\n",
    "    print(f\"Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'batch_size': 16,\n",
       "   'dropout_rate': 0,\n",
       "   'filters': 32,\n",
       "   'kernel_size': 3,\n",
       "   'learning_rate': 0.1,\n",
       "   'strides': 1},\n",
       "  [16194.947056189074,\n",
       "   3292.561664282486,\n",
       "   3288.8790442503387,\n",
       "   3286.0267736976107,\n",
       "   3285.320848820859,\n",
       "   3284.8166698140776,\n",
       "   3285.527627116817,\n",
       "   3283.790940905592,\n",
       "   3283.5548529387106,\n",
       "   3285.022400261668,\n",
       "   3282.410796245784,\n",
       "   3281.528735421431,\n",
       "   3283.213966939806,\n",
       "   3281.4881408841125,\n",
       "   3280.3637024175314,\n",
       "   3278.605589203355,\n",
       "   3275.4491643723172,\n",
       "   3275.0328355865604,\n",
       "   3275.1365915892657,\n",
       "   3275.8974027270287,\n",
       "   3275.3032936227196,\n",
       "   3275.1655106152266,\n",
       "   3275.1831530392415,\n",
       "   3275.435699637562,\n",
       "   3275.393238662133,\n",
       "   3275.246429009176,\n",
       "   3275.1776739686607,\n",
       "   3275.4425212520123,\n",
       "   3275.4410285906038,\n",
       "   3275.138917522789,\n",
       "   3275.169674101211,\n",
       "   3275.4949075918753,\n",
       "   3275.170228128753,\n",
       "   3275.4271265786297,\n",
       "   3275.3113789294066,\n",
       "   3275.2178020436745,\n",
       "   3275.3286345512283,\n",
       "   3275.5042083415847,\n",
       "   3275.618457500367,\n",
       "   3275.3624273966107,\n",
       "   3275.231435056797,\n",
       "   3275.688985444987,\n",
       "   3275.2787324669903,\n",
       "   3274.953348858787,\n",
       "   3275.4638582942334,\n",
       "   3275.315084397198,\n",
       "   3275.885909283093,\n",
       "   3275.223351990894,\n",
       "   3275.038233340147,\n",
       "   3275.405109735522],\n",
       "  3251.3730548187514),\n",
       " ({'batch_size': 16,\n",
       "   'dropout_rate': 0,\n",
       "   'filters': 32,\n",
       "   'kernel_size': 3,\n",
       "   'learning_rate': 0.1,\n",
       "   'strides': 2},\n",
       "  [2075.9246832460044,\n",
       "   3289.837349413344,\n",
       "   3287.7571802496423,\n",
       "   3285.6245015478876,\n",
       "   3286.0489846904065,\n",
       "   3285.220029901737,\n",
       "   3285.0008988287636,\n",
       "   3281.719840408288,\n",
       "   3282.2276308697556,\n",
       "   3281.5081538577015,\n",
       "   3280.7053784898335,\n",
       "   3278.9034776813946,\n",
       "   3275.9930923674015,\n",
       "   3275.2720947964312,\n",
       "   3275.127527314605,\n",
       "   3275.2059420793835,\n",
       "   3275.1662179542973,\n",
       "   3275.1462312013714,\n",
       "   3275.0779686042742,\n",
       "   3275.4960286125624,\n",
       "   3275.5087670139865,\n",
       "   3275.1799395357693,\n",
       "   3275.2784772372306,\n",
       "   3275.763542598247,\n",
       "   3275.0280037481475,\n",
       "   3275.3150697647307,\n",
       "   3275.2259094587625,\n",
       "   3275.3608750175667,\n",
       "   3275.1200909169306,\n",
       "   3275.530335959858,\n",
       "   3275.295694571095,\n",
       "   3275.5040068107846,\n",
       "   3274.9872168328075,\n",
       "   3275.417703768589,\n",
       "   3275.0490303145602,\n",
       "   3274.8776944401416,\n",
       "   3275.452300955522,\n",
       "   3275.2771712246235,\n",
       "   3275.3855140966757,\n",
       "   3275.3580779305657,\n",
       "   3275.6585235221405,\n",
       "   3275.2066900719606,\n",
       "   3275.1430634917338,\n",
       "   3275.390797710029,\n",
       "   3275.2588197946743,\n",
       "   3275.3776209983293,\n",
       "   3275.4391487976222,\n",
       "   3275.622995621439,\n",
       "   3275.320784961017,\n",
       "   3275.4966654443],\n",
       "  3249.4720889703367),\n",
       " ({'batch_size': 16,\n",
       "   'dropout_rate': 0,\n",
       "   'filters': 32,\n",
       "   'kernel_size': 3,\n",
       "   'learning_rate': 0.01,\n",
       "   'strides': 1},\n",
       "  [1826.6864618259283,\n",
       "   1601.9817254029817,\n",
       "   1521.8903304142925,\n",
       "   1467.7718213060305,\n",
       "   1424.4179376335487,\n",
       "   1389.8201382553586,\n",
       "   1344.0140808267663,\n",
       "   1315.314065881138,\n",
       "   1282.1323807398999,\n",
       "   1256.498662828522,\n",
       "   1232.6297531327555,\n",
       "   1216.0987636088935,\n",
       "   1189.395764706628,\n",
       "   1173.2068770676096,\n",
       "   1154.605734714561,\n",
       "   1143.144227828418,\n",
       "   1125.5742124855371,\n",
       "   1106.8867498452412,\n",
       "   1097.8777173383062,\n",
       "   1086.134820626509,\n",
       "   1076.285226532756,\n",
       "   1062.734398722863,\n",
       "   1057.0119151155186,\n",
       "   1042.1607983094011,\n",
       "   1029.021431146405,\n",
       "   1024.1112723050308,\n",
       "   1014.0946812777079,\n",
       "   1011.3299015123144,\n",
       "   1005.1803781105472,\n",
       "   995.7792554379679,\n",
       "   989.049250097267,\n",
       "   981.9823889656722,\n",
       "   974.9928673196015,\n",
       "   971.2471539613563,\n",
       "   968.5239697859507,\n",
       "   958.9630267675841,\n",
       "   958.14817424534,\n",
       "   950.7459115777752,\n",
       "   945.9632056896743,\n",
       "   941.4610719832064,\n",
       "   941.9449703724971,\n",
       "   925.1305407882029,\n",
       "   925.1584685393464,\n",
       "   922.1413328980191,\n",
       "   920.3729946956806,\n",
       "   913.8744238740386,\n",
       "   909.8170368472182,\n",
       "   903.6094290046349,\n",
       "   900.4535083811301,\n",
       "   899.8561556824678],\n",
       "  963.7732616347218),\n",
       " ({'batch_size': 16,\n",
       "   'dropout_rate': 0,\n",
       "   'filters': 32,\n",
       "   'kernel_size': 3,\n",
       "   'learning_rate': 0.01,\n",
       "   'strides': 2},\n",
       "  [1818.9105723869966,\n",
       "   1661.0004412023336,\n",
       "   1610.139758667591,\n",
       "   1567.996471937153,\n",
       "   1531.4236084508389,\n",
       "   1501.2639828299073,\n",
       "   1475.879113906773,\n",
       "   1441.4102165490413,\n",
       "   1411.12867027526,\n",
       "   1390.001661663414,\n",
       "   1364.3249555461443,\n",
       "   1348.2045018855022,\n",
       "   1328.228003778091,\n",
       "   1303.7038505148712,\n",
       "   1286.820162364994,\n",
       "   1270.1149649020404,\n",
       "   1246.1709736035675,\n",
       "   1228.1387046059135,\n",
       "   1220.6354731961342,\n",
       "   1206.221018416747,\n",
       "   1190.822560911888,\n",
       "   1171.8919724246857,\n",
       "   1176.3412998938697,\n",
       "   1159.6098622132595,\n",
       "   1149.4278598264505,\n",
       "   1139.830500822153,\n",
       "   1129.3832109652478,\n",
       "   1121.1851865916592,\n",
       "   1113.4397316958411,\n",
       "   1105.366537295924,\n",
       "   1106.2287412878927,\n",
       "   1090.9873020287528,\n",
       "   1086.3977135683997,\n",
       "   1073.6490541908845,\n",
       "   1076.8187221935705,\n",
       "   1064.8187356307872,\n",
       "   1056.9070870836933,\n",
       "   1052.3996440310411,\n",
       "   1051.000207014232,\n",
       "   1040.8878561486208,\n",
       "   1038.8100784339188,\n",
       "   1027.2328156103974,\n",
       "   1030.8588257883966,\n",
       "   1025.1939023084165,\n",
       "   1024.9648284363143,\n",
       "   1008.2241398721666,\n",
       "   1014.4089542778808,\n",
       "   1001.4287998102769,\n",
       "   1001.2376432005539,\n",
       "   1003.0874047804152],\n",
       "  1030.7126973802706),\n",
       " ({'batch_size': 16,\n",
       "   'dropout_rate': 0,\n",
       "   'filters': 32,\n",
       "   'kernel_size': 3,\n",
       "   'learning_rate': 0.001,\n",
       "   'strides': 1},\n",
       "  [1929.794294712569,\n",
       "   1640.720396943611,\n",
       "   1546.2343124227455,\n",
       "   1470.6486483607443,\n",
       "   1396.6069336411608,\n",
       "   1319.150710458202,\n",
       "   1240.9066905211155,\n",
       "   1171.4852673996108,\n",
       "   1107.1684767815877,\n",
       "   1051.421795786548,\n",
       "   1007.5655838711381,\n",
       "   962.48430226935,\n",
       "   927.2728272644691,\n",
       "   897.108023750012,\n",
       "   866.9396981171478,\n",
       "   839.3847758644833,\n",
       "   817.4015655792061,\n",
       "   798.2516855060518,\n",
       "   780.5892729240661,\n",
       "   758.4712023855913,\n",
       "   739.6516030823901,\n",
       "   725.4757990745862,\n",
       "   710.0588812384368,\n",
       "   694.8778339145813,\n",
       "   683.7628900061644,\n",
       "   674.0641390522094,\n",
       "   663.2248392087947,\n",
       "   648.7654513292788,\n",
       "   637.7734417769525,\n",
       "   632.1619801138428,\n",
       "   620.6041462534096,\n",
       "   612.2657473806661,\n",
       "   601.1537401949491,\n",
       "   595.4722092866313,\n",
       "   585.7108484931003,\n",
       "   578.3577780710316,\n",
       "   572.132189464803,\n",
       "   564.655863190945,\n",
       "   560.0132721814384,\n",
       "   552.0482795447086,\n",
       "   546.6960371059564,\n",
       "   541.081366662987,\n",
       "   534.9201667380157,\n",
       "   530.1589825892975,\n",
       "   526.2350361453639,\n",
       "   516.2132263604676,\n",
       "   513.1381300788187,\n",
       "   508.3267939813197,\n",
       "   506.3760124125446,\n",
       "   500.15017608056183],\n",
       "  558.7647701685065),\n",
       " ({'batch_size': 16,\n",
       "   'dropout_rate': 0,\n",
       "   'filters': 32,\n",
       "   'kernel_size': 3,\n",
       "   'learning_rate': 0.001,\n",
       "   'strides': 2},\n",
       "  [1983.8945560508348,\n",
       "   1661.2308393726364,\n",
       "   1601.6934261443669,\n",
       "   1556.5379019911136,\n",
       "   1515.2419703835264,\n",
       "   1467.5222161555816,\n",
       "   1418.3317248864537,\n",
       "   1364.3948767024965,\n",
       "   1312.0264984929356,\n",
       "   1258.4119082284774,\n",
       "   1204.7382619750178,\n",
       "   1149.9696648810598,\n",
       "   1104.614756215954,\n",
       "   1062.8512169963496,\n",
       "   1026.158814343681,\n",
       "   992.8152612144988,\n",
       "   959.6108269567482,\n",
       "   930.9896909849427,\n",
       "   905.5716716463802,\n",
       "   887.2029757670467,\n",
       "   863.6616445664588,\n",
       "   844.4929576957995,\n",
       "   830.7664730343386,\n",
       "   809.0771696638105,\n",
       "   793.2650981843813,\n",
       "   780.5518019407183,\n",
       "   768.5297366924395,\n",
       "   757.0740954544929,\n",
       "   746.4528938062641,\n",
       "   730.0076606881491,\n",
       "   725.3379125655916,\n",
       "   714.6470466757249,\n",
       "   703.1216449372611,\n",
       "   695.5561927620739,\n",
       "   689.2197403459401,\n",
       "   675.5515865734534,\n",
       "   669.8392735928078,\n",
       "   662.693924637028,\n",
       "   656.502401067152,\n",
       "   651.8077659145303,\n",
       "   643.4717020067541,\n",
       "   636.6065903888992,\n",
       "   629.2724887270862,\n",
       "   624.6615472483226,\n",
       "   613.8071375805534,\n",
       "   612.0019548068354,\n",
       "   603.7262280985847,\n",
       "   601.3204749549498,\n",
       "   595.2993690659156,\n",
       "   589.4567629922537],\n",
       "  661.1930266505678),\n",
       " ({'batch_size': 16,\n",
       "   'dropout_rate': 0,\n",
       "   'filters': 32,\n",
       "   'kernel_size': 3,\n",
       "   'learning_rate': 0.0001,\n",
       "   'strides': 1},\n",
       "  [2927.9568339612188,\n",
       "   1812.3677630520413,\n",
       "   1768.1720858357914,\n",
       "   1739.0794149203857,\n",
       "   1711.6339376154826,\n",
       "   1685.5078913316704,\n",
       "   1659.5409685560649,\n",
       "   1642.1351543111482,\n",
       "   1624.0151971425566,\n",
       "   1609.9787590095477,\n",
       "   1595.0502904435016,\n",
       "   1583.9781255265093,\n",
       "   1570.6734781734688,\n",
       "   1558.899047275147,\n",
       "   1548.0935351293008,\n",
       "   1535.3041020415985,\n",
       "   1525.3335814481472,\n",
       "   1511.87174505201,\n",
       "   1502.2681784454371,\n",
       "   1491.5643567513366,\n",
       "   1478.0074142468532,\n",
       "   1465.1189443019016,\n",
       "   1455.0223508521367,\n",
       "   1438.896320725423,\n",
       "   1429.5451429041116,\n",
       "   1417.7369954705725,\n",
       "   1404.5611490793922,\n",
       "   1391.7942095166236,\n",
       "   1378.8725060763168,\n",
       "   1368.6517383341504,\n",
       "   1351.6624027629612,\n",
       "   1340.6566503442123,\n",
       "   1326.9867882629544,\n",
       "   1315.375096493438,\n",
       "   1300.7207263837363,\n",
       "   1288.9423654551595,\n",
       "   1272.1357968496477,\n",
       "   1258.665573375886,\n",
       "   1246.1743096090397,\n",
       "   1233.6469299778037,\n",
       "   1217.7819297547235,\n",
       "   1203.8728905940582,\n",
       "   1192.865226697571,\n",
       "   1178.0973313338623,\n",
       "   1165.5997327091843,\n",
       "   1148.7131198711504,\n",
       "   1134.1840739227569,\n",
       "   1123.1317409175394,\n",
       "   1106.8114844096847,\n",
       "   1093.8743380105993],\n",
       "  1114.8685647179832),\n",
       " ({'batch_size': 16,\n",
       "   'dropout_rate': 0,\n",
       "   'filters': 32,\n",
       "   'kernel_size': 3,\n",
       "   'learning_rate': 0.0001,\n",
       "   'strides': 2},\n",
       "  [3299.791910361464,\n",
       "   2034.5481436703699,\n",
       "   1760.1752170615769,\n",
       "   1736.6843984833138,\n",
       "   1720.469884892087,\n",
       "   1704.4606639372357,\n",
       "   1695.06180666458,\n",
       "   1682.5880172417578,\n",
       "   1671.311985041446,\n",
       "   1657.8096640461308,\n",
       "   1650.896345231812,\n",
       "   1639.5397065007483,\n",
       "   1629.2762669136753,\n",
       "   1620.589987868826,\n",
       "   1611.9565012223002,\n",
       "   1601.5717377507483,\n",
       "   1595.694042117851,\n",
       "   1589.0742006191151,\n",
       "   1581.5811785893663,\n",
       "   1574.6431284195658,\n",
       "   1568.9963734062987,\n",
       "   1560.6373650439334,\n",
       "   1553.5381315142429,\n",
       "   1545.900044047121,\n",
       "   1542.6026151094102,\n",
       "   1534.5496990063496,\n",
       "   1531.2832038112088,\n",
       "   1523.369227696167,\n",
       "   1518.1912760250314,\n",
       "   1509.7326955275953,\n",
       "   1503.3704494535582,\n",
       "   1497.2384031755848,\n",
       "   1491.921228830832,\n",
       "   1482.7033268115215,\n",
       "   1475.2514118908862,\n",
       "   1467.9917738308598,\n",
       "   1462.0961636434884,\n",
       "   1455.5421290499796,\n",
       "   1448.5402272184658,\n",
       "   1440.8134912523597,\n",
       "   1435.296873739871,\n",
       "   1426.7625687107138,\n",
       "   1421.1160649995968,\n",
       "   1410.9060241000534,\n",
       "   1406.1580479494783,\n",
       "   1400.4538070691187,\n",
       "   1392.9143667600044,\n",
       "   1385.9173553067549,\n",
       "   1379.2109463932663,\n",
       "   1372.3355516335464],\n",
       "  1369.0989089975799)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to a file\n",
    "results_file = \"pytorch_grid_search_results.txt\"\n",
    "\n",
    "with open(results_file, 'w') as file:\n",
    "    file.write(\"Parameters | Train Loss| Test Loss\\n\")\n",
    "    file.write(\"-\" * 50 + \"\\n\")\n",
    "    for params, train_loss, test_loss in results:\n",
    "        param = \"\"\n",
    "        for item,val in params.items():\n",
    "            param += f\"{str(item)}:{str(val)}, \"\n",
    "        file.write(f\"{param} | {train_loss[-1]:.4f} | {test_loss:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read results from the file\n",
    "results_df = pd.read_csv(results_file, sep=\"|\", skipinitialspace=True)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(results_df.index, results_df['Validation Accuracy'], label='Validation Accuracy', marker='o')\n",
    "plt.plot(results_df.index, results_df['Test Accuracy'], label='Test Accuracy', marker='x')\n",
    "plt.xlabel('Parameter Combinations')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Comparison of Validation and Test Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(results_df.index, results_df['Parameters'], rotation=45, ha='right', fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlrat_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
